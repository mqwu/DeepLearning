Resource

Good intro.
https://medium.freecodecamp.org/want-to-know-how-deep-learning-works-heres-a-quick-guide-for-everyone-1aedeca88076


Softmax
https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html

2017 AI frontier conference slides
https://www.slideshare.net/AIFrontiers/presentations

Glossary
https://deeplearning4j.org/glossary

NLP
https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f

Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning
https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3Bm1QWJyqkT9OpftbslbFxCw%3D%3D

LSTM
http://colah.github.io/posts/2015-08-Understanding-LSTMs/

Reinforcement Learning
http://rll.berkeley.edu/deeprlcourse/
https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3

https://classroom.udacity.com/courses/ud600


Gradient clipping
Gradient clipping is most common in recurrent neural networks. When gradients are being propagated back in time, they can vanish because they they are continuously multiplied by numbers less than one. This is called the vanishing gradient problem. This is solved by LSTMs and GRUs, and if you’re using a deep feedforward network, this is solved by residual connections. On the other hand, you can have exploding gradients too. This is when they get exponentially large from being multiplied by numbers larger than 1. Gradient clipping will clip the gradients between two numbers to prevent them from getting too large.

