#############
# NN and DL
#############

## Intro
-Standard NN: predictive modelling
-CNN: image classification
-RNN: sequence data, such as Audio and language translation
-Custom/Hybrid NN: auto drive car(in: image, Radar info out:positio of other cars)

-Unstructure data: image, audio, text...

-Scale drive the DL, more data improve performance, but not in traditional ML 


## Basic of NN programming
# Logistic regression as a very small NN
X.shape = (n_x, m)
y.shape = (1,m)

=logistic regression (y = prob of success)
--y_hat = sigma(W^TX + b)  
--sigma fun = 1/(1+e^-x)

=Cost function 
-Loss function (this is for single training sample)
--L(y,y_hat) = 0.5*(y_hat - y)^2   (non-convex, usually do not use it for logistic reg)
--L(y,y_hat) = -[ylogy_hat +(1-y)log(1-y_hat)] (convex function)
-Cost function (for all training data)
--J(w,b)=1/m sum[L(y_i,y_hat_i)] i=1,...,m 
--Minize J to find (w,b)

=Gradient decent
-w = w - alpha x dJ/dw   (alpha: learning rate)
-b = b - alpha x dJ/db   

=Computation graph
-forward propogation to calculate function value
-backward propogation to calculate derivative (dLoss/dx using chain rule)

=Vectorization
- Z = np.dot(w.T, x) + b    Z.shape=(1,m)
- A = sigma(Z)
- dZ = A - Y
- dw = 1/m X dZ.T
- db = 1/m np.sum(dZ)
- w = w - alpha dw
- b = b - alpha db

- u = np.exp(v)  =np.abs(v)  =np.log(v) =np.maximize(v)
- dw = np.zeros(n_x, 1)  dw += x^(i)dz^(i)  dw /= m  #sample i = 1, ..., m



## Shallow NN (One hidden layer NN or 2 layer NN)
= Input X 
a^[0] = X

= Hidden Layer 1
- Z^[1] = W^[1]X + b^[1]
- a^[1] = sigma(Z^[1])

= Output Layer 
- Z^[2] = W^[2]a^[1] + b^[2]
- a^[2] = sigma(Z^[2])

- y_hat = a^[2]
- L(a^[2], y)

= Each node has Two step computation (linear + non-linear)
- Z = W^TX + b    W.shape(# of node, # of input)
- y = sigma(Z)

-Z^[1] = W^[1]X + b^[1] = W^[1]A^[0] + b^[1]
-A^[1] = sigma(Z^[1])
-Z^[2] = W^[2]A^[1] + b^[2]
-A^[2] = sigma(Z^[2])

= Activation function (Non-linear)
- sigmoid = 1/(1+e^-z)    (0, 1)
---- use only if it is for binary classification
- tanh = shift sigmoid = (e^z - e^-z)/(e^z + e^-z)  (-1, 1)
---- when z is very large or small, the gradient(derivative) become very small

-ReLu = max(0, z)
---- z <= 0, derivative = 0
---- z > 0, derivative = 1
---- since most of node z may > 0, the learning is fast

- How to choose Activation function
--Default is ReLU
--But you may experiment different Activation function and use CV to pick the best one
--Hidden layer use ReLu, output layer use linear activation for continuous output
--All layer use ReLu for >0 output

- Why you need activation function  
--b/c composition of linear will always gives you linear function

= Randomly initialize Weight
--If not you will end up with exact same function for each hiddent unit
-- W should be initialize with a random small number (close to 0)
----W^[1] = np.random.randn((2,2)) * 0.01  (if x 1000, z will be large, derivative very small for activation ftn)

= Parameters Dim
- W[i], dW[i]: (n[i], n[i-1])             n[i]: # of node in layer i
- b[i], db[i]: (n[i], m)                     m: # of samples
- Z[i], A[i], dZ[i], dA[i]: (n[i], m)     i=0, A[0]=X    

= Why deep
- early layer represent simple functions (low level features)
  later layer represent complex functions building from early layer (simple functions)
- same complexity shallow layer require exponentially more hidden units, that's why DL is prefered

= Building blocks
- forward propagation
--In: A[i-1] Out: A[i]
-- Z[i] = W[i]A[i-1] + b[i]   # cash Z[i] W[i] b[i] for backward propagation
-- A[i] = g_i(Z[i])

- backward propagation
--In: dA[i]  cash (Z[i], W[i], b[i])    Out: dA[i-1], dW[i], db[i]
-- dZ[i] = dA[i]*g[i]'(Z[i])
-- dW[i] = 1/m*dZ[i]*A[i-1].T
-- db[i] = 1/m*np.sum(dZ[i], axis=1, keepdim=True)
-- dA[i-1] = W[i].T*dZ[i]

= Hyper parameters
- learning rate alpha
- # of iterations
- # hidden layers
- # of hidden units
- choice of activation functions

- momentum
- minibatch size
- regularization par

- Empirical process to tune these paras


## Deep NN
